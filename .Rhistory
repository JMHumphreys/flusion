# Chunk 1
options(dplyr.summarise.inform = FALSE)
#wrangling
library(tidyverse)
library(lubridate)
library(data.table, include.only = "fread")
library(cdcfluview)
library(yaml)
#spatial
library(sp)
library(sf)
library(spdep)
library(rgeos)
library(igraph)
library(maptools)
library(mapproj)
library(CovidCAR)
#devtools::install_github("JMHumphreys/CovidCAR")
#messages
library(cli)
#inference
library(INLA)
#Utilities
source("./R/utilities.R")
# Chunk 2
myRegions <- surveillance_areas()
flusurv_all <- do.call(rbind, lapply(seq_len(dim(myRegions)[1]), function(i) {
hospitalizations(surveillance_area = myRegions$surveillance_area[i], region = myRegions$region[i])
}))
# Chunk 3
range(flusurv_all$year)
flusurv <- flusurv_all %>%
filter(age_label == "Overall",
region != "Entire Network",
year >= 2010 & year <= 2019) %>% #the pkg fails on dates after 2020,ugh
mutate(location_name = region,
network = surveillance_area,
weeklyrate = as.numeric(weeklyrate),
epiweek = year_wk_num) %>%
select(location_name, year, epiweek, network, rate, weeklyrate)
#manual download from site 2023-06-01
flusurv_2020 <- fread("./data/FluSurveillance_2020.csv") %>%
rename_all(~gsub(" |-", "", .)) %>%
filter(AGECATEGORY == "Overall",
SEXCATEGORY == "Overall",
RACECATEGORY == "Overall",
CATCHMENT != "Entire Network",
MMWRYEAR >= 2020) %>% #Prior to this date was downloaded in code above
mutate(location_name = CATCHMENT,
network = NETWORK,
year = MMWRYEAR,
epiweek = MMWRWEEK,
rate = CUMULATIVERATE,
weeklyrate = as.numeric(WEEKLYRATE)) %>%
select(location_name, year, epiweek, network, rate, weeklyrate)
#Join date ranges and scale weeklyrate
flusurv = rbind(flusurv, flusurv_2020)
flusurv$weeklyrate.s = as.numeric(scale(flusurv$weeklyrate, scale = T, center=T))
#combine NY data
flusurv$location_name[flusurv$location_name == "New York - Albany"] = "New York"
flusurv$location_name[flusurv$location_name == "New York - Rochester"] ="New York"
flusurv <- flusurv %>%
group_by(location_name, year, epiweek) %>%
summarise(rate = mean(rate, na.rm=T),
weeklyrate = mean(weeklyrate, na.rm=T),
weeklyrate.s = mean(weeklyrate.s, na.rm=T))
#Check for duplicates
unique(duplicated(flusurv))
dim(flusurv)
head(flusurv)
# Chunk 4
flusurv_en <- flusurv_all %>%
filter(age_label == "Overall",
region == "Entire Network",
year >= 2010 & year <= 2019) %>% #the pkg fails on dates after 2020,ugh
mutate(location_name = region,
network = surveillance_area,
epiweek = year_wk_num) %>%
select(location_name, year, epiweek, network, rate, weeklyrate)
flusurv_en2020 <- fread("./data/FluSurveillance_2020.csv") %>%
rename_all(~gsub(" |-", "", .)) %>%
filter(AGECATEGORY == "Overall",
SEXCATEGORY == "Overall",
RACECATEGORY == "Overall",
CATCHMENT == "Entire Network",
MMWRYEAR >= 2020) %>% #the pkg fails on dates after 2020,ugh
mutate(location_name = CATCHMENT,
network = NETWORK,
year = MMWRYEAR,
epiweek = MMWRWEEK,
rate = CUMULATIVERATE,
weeklyrate = WEEKLYRATE) %>%
select(location_name, year, epiweek, network, rate, weeklyrate)
#Join
flusurv_en <- rbind(flusurv_en, flusurv_en2020)
unique(duplicated(flusurv_en))
dim(flusurv_en)
head(flusurv_en)
# Chunk 5
ilinet <- ilinet(region = "state") %>%
mutate(location_name = region,
epiweek = week,
unweighted = as.numeric(unweighted_ili),
unweighted.s = unweighted,
total = ilitotal,
providers = num_of_providers) %>%
select(location_name, year, epiweek, unweighted, unweighted.s, total, providers)
# Clip to between 0.0001 and 99.999
ilinet$unweighted.s <- pmin(pmax(as.numeric(ilinet$unweighted.s), 0.0001), 99.9999)/100
# logit transform
ilinet$unweighted.s <- log(ilinet$unweighted.s/(1-ilinet$unweighted.s))
range(ilinet$unweighted.s, na.rm=T)
unique(duplicated(ilinet))
dim(ilinet)
range(ilinet$year)
head(ilinet)
# Chunk 6
url <- "https://healthdata.gov/api/views/g62h-syeh/rows.csv?accessType=DOWNLOAD"
filename <- "./data/flu_HHS.csv"
download_file(url, filename)
# Chunk 7
flu_HHS <- fread("./data/flu_HHS.csv") %>%
mutate(abbreviation = state,
date = as_date(date) - 1, #1-day prior, per fluSight truth
year = year(date),
epiweek = epiweek(date)) %>%
group_by(abbreviation, year, epiweek) %>%
summarise(hosp_inc = sum(previous_day_admission_influenza_confirmed))
unique(duplicated(flu_HHS))
dim(flu_HHS)
range(flu_HHS$year)
head(flu_HHS)
# Chunk 8
nrevss.1 <- fread("./data/WHO_NREVSS_Combined_prior_to_2015_16.csv") %>%
rename_all(~gsub(" |-", "", .)) %>%
mutate(location_name = REGION,
year = YEAR,
epiweek = WEEK,
tot_perc = as.numeric(PERCENTPOSITIVE),
Bpos = as.numeric(B),
tot_samp = as.numeric(TOTALSPECIMENS),
Apos = tot_perc - ((Bpos/tot_samp)*100)) %>%
select(location_name, year, epiweek, Apos)
nrevss.2 <- fread("./data/WHO_NREVSS_Clinical_Labs.csv") %>%
rename_all(~gsub(" |-", "", .)) %>%
mutate(location_name = REGION,
year = YEAR,
epiweek = WEEK,
Apos = PERCENTA) %>%
select(location_name, year, epiweek, Apos) #Apos = Influenza A positive
# Combine
nrevss <- rbind(nrevss.1, nrevss.2)
# Replace "X"
nrevss$Apos[nrevss$Apos == "X"] <- NA
# Clip to between 0.0001 and 99.999
nrevss$Apos <- pmin(pmax(as.numeric(nrevss$Apos), 0.0001), 99.9999)/100
# logit transform
nrevss$Apos.s <- log(nrevss$Apos/(1-nrevss$Apos))
unique(duplicated(nrevss))
head(nrevss)
# Chunk 9
url <- "https://github.com/cdcepi/Flusight-forecast-data/raw/master/data-locations/locations.csv"
filename <- "./data/locations.csv"
download_file(url, filename)
# Chunk 10
locations <- fread("./data/locations.csv") %>%
select(-c(count_rate1per100k, count_rate2per100k)) %>%
filter(location_name != "US") #remove aggregate group
head(locations)
# Chunk 11
myYears <- seq(2010, 2023, by = 1)
week_nums <- 1:52
year_set <- lapply(myYears, function(year) {
tmp_frame <- locations %>% mutate(year = year)
weekly_set <- lapply(week_nums, function(week_num) {
tmp_frame_wk <- tmp_frame %>% mutate(epiweek = week_num)
return(tmp_frame_wk)
})
weekly_set <- do.call(rbind, weekly_set)
return(weekly_set)
})
template <- do.call(rbind, year_set)
dim(template)
head(template) #all states and times represented
#most_recent <- max(subset(flu_HHS, year == 2023)$epiweek) #drop future dates
most_recent <- 22 #models were initiall fit with 2023-Epiweek-22
template$keep <- ifelse(template$year == 2023 & template$epiweek > most_recent, "drop", "keep")
template <- template %>%
filter(keep == "keep") %>%
select(-keep)
unique(duplicated(template))
# Chunk 12
template <- template %>% filter(year >= 2010)
# Chunk 13
template <- template %>%
arrange(year, epiweek) %>%
mutate(ts_week = as.integer(as.factor(year + (epiweek/53))))
range(template$ts_week) #number of epiweeks
# Chunk 14
setup_analysis(report_date = "2010-01-01",
training_period = 2*28, #days
forecast_horizon = 28, #days
output_dir = file.path(getwd(), "data")
)
# Chunk 15
States <- download_boundaries(unit = "state")
class(States)
head(States@data[,c("Region", "State")]) #appended attributes
# Chunk 16
nb_flusion = get_neighbors(States, connect=TRUE)
summary(nb_flusion)
#view
plot_neighbors(States, nb_flusion)
#convert to matrix
nb2INLA("J", nb_flusion)
J = inla.read.graph("J")
# Chunk 17
template$Region =  with(States@data[,c("Region", "State")],
Region[match(
template$location_name,
State)])
# Chunk 18
en_match <- flusurv_en %>%
group_by(year, epiweek) %>%
summarise(en_est = mean(as.numeric(weeklyrate), na.rm=TRUE)) %>%
select(year, epiweek, en_est)
# Clip to between 0.0001 and 99.999
en_match$en_est.s <- pmin(pmax(as.numeric(en_match$en_est), 0.0001), 99.9999)/100
# logit transform
en_match$en_est.s <- round(log(en_match$en_est.s/(1-en_match$en_est.s)), 3)
range(en_match$en_est.s, na.rm=T)
template <- left_join(template, en_match, by = c("year", "epiweek"))
unique(duplicated(template))
# Chunk 19
template$st_int <- as.integer(as.factor(paste0(template$Region, ".", template$ts_week)))
range(template$st_int)
# Chunk 20
#FluSurv
flusurv_full <- left_join(template, flusurv, by = c("location_name", "year", "epiweek"))
flusurv_full$network[is.na(flusurv_full$network)] = "none"
unique(duplicated(flusurv_full))
head(flusurv_full) #times and locations w/out values assigned NA
#ILI Surveillance
ilinet_full <- left_join(template, ilinet, by = c("location_name", "year", "epiweek"))
unique(duplicated(ilinet_full))
head(ilinet_full)
#HHS
flu_HHS_full <- left_join(template, flu_HHS, by = c("abbreviation", "year", "epiweek"))
unique(duplicated(flu_HHS_full))
unique(duplicated(flu_HHS_full[,c("ts_week", "Region")]))
head(flu_HHS_full)
#nrevss
nrevss_full <- left_join(template, nrevss, by = c("location_name", "year", "epiweek"))
unique(duplicated(nrevss_full))
head(nrevss_full)
# Chunk 21
fs_plt <- flusurv_full %>%
mutate(value = weeklyrate.s,
set = "FluSurv") %>%
select(location_name, ts_week, value, set)
ili_plt <- ilinet_full %>%
mutate(value = unweighted.s,
set = "ILI") %>%
select(location_name, ts_week, value, set)
hhs_plt <- flu_HHS_full %>%
mutate(value = log(hosp_inc+0.0001),
set = "HHS") %>%
select(location_name, ts_week, value, set)
nrevss_plt <- nrevss_full %>%
mutate(value = Apos.s,
set = "NREVSS") %>%
select(location_name, ts_week, value, set)
all_plts <- rbind(fs_plt, ili_plt, hhs_plt, nrevss_plt)
all_plts$set <- ordered(factor(all_plts$set), levels = c("FluSurv", "ILI", "NREVSS", "HHS"))
ggplot() +
geom_tile(data=all_plts,
aes(ts_week, location_name, fill = value)) +
xlab(" ") +
viridis::scale_fill_viridis(paste0(" "),
discrete=F,
option = "turbo",
direction = -1,
na.value = "white") +
ylab("Location") +
xlab("Weekly Timesteps (2010-2023)") +
facet_wrap(~set, ncol = 4) +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
panel.background = element_blank(),
plot.background = element_blank(),
panel.border = element_blank(),
legend.title = element_text(size = 16, face = "bold", hjust=0.5),
legend.text = element_text(size=10, face="bold"),
strip.text = element_text(size=16, face="bold"),
strip.background = element_blank(),
legend.position="none",
legend.direction = "horizontal",
legend.box = "horizontal",
axis.text.y = element_text(face="bold", size=5),
axis.text.x = element_text(face="bold", size=12, vjust=0.5,
hjust=1, angle=90),
axis.title.x = element_text(size=12, face="bold"),
axis.title.y = element_text(size=18, face="bold"),
plot.title = element_text(size=18, face="bold", hjust=0.5)) +
guides(color = guide_legend(title.position = "top", label.position = "bottom"))
# Chunk 22
#save(list=c("nrevss_full", "J", "ilinet_full", "flu_HHS_full"), file="./data/prepro_06072023.RData", version = 2)
save(list=c("nrevss_full", "J", "ilinet_full", "flu_HHS_full"), file="./data/prepro_06072023.RData", version = 2)
#Utilities
source("./R/utilities.R")
#Utilities
source("./R/utilities.R")
#Preprocessing
load("./data/prepro_06072023.RData")
load("D:/Github/flusion_model/data/prepro_06072023.RData")
getwd()
file.path(getwd(), /data/FluSurveillance_2020.csv)
file.path(getwd(), "data/FluSurveillance_2020.csv"")
)
ile.path(getwd(), "data/FluSurveillance_2020.csv")
file.path(getwd(), "data/FluSurveillance_2020.csv")
file.path(getwd(), "data")
getwd()
library(plotly)
# helper function to get samples from fixed effect posteriors
get_fixed_samples <- function(mod=r1, ss=10) {
fe <- c(rownames(mod$summary.fixed))
idx <- as.list(rep(0, length(fe)))
names(idx) <- fe
samp <- inla.posterior.sample(n=ss, result=mod, selection=idx)
samp <- t(as.data.frame(lapply(samp, function(x) x$latent)))
samp <- as.matrix(as.data.frame(samp))
colnames(samp) <- fe
rownames(samp) <- NULL
return(samp)
}
# make data with random intercept and two interacting continuous fixed effects
ngroups <- 10
nobs <- 100
beta0 <- 5
beta1 <- 1
beta2 <- 2
beta3 <- -3
dat <- data.frame(x1 = runif(nobs, -1, 1),
x2 = runif(nobs, -1, 1),
mygroup = rep(seq_len(ngroups), nobs / ngroups)) %>%
mutate(y = beta0 +
(beta1 * x1) +
(beta2 * x2) +
(beta3 * x1 * x2) +
(rnorm(ngroups)[mygroup]) +
rnorm(nobs, sd = 0.1),
x2_cat=factor(cut(x2, breaks = 5)))
summary(dat)
# plot it to see interaction in generated data
ggplot(dat, aes(x1, y, group=mygroup)) + geom_point() +
geom_smooth(method="lm", se=F) + facet_wrap(~x2_cat)
# run inla model and specify config=T for posterior sampling
form1 <- y ~ 1 + x1 * x2 + f(mygroup, model="iid")
res1 <- inla(form1, data = as.data.frame(dat),
control.compute=list(config=T))
library(INLA)
library(tidyverse)
# helper function to get samples from fixed effect posteriors
get_fixed_samples <- function(mod=r1, ss=10) {
fe <- c(rownames(mod$summary.fixed))
idx <- as.list(rep(0, length(fe)))
names(idx) <- fe
samp <- inla.posterior.sample(n=ss, result=mod, selection=idx)
samp <- t(as.data.frame(lapply(samp, function(x) x$latent)))
samp <- as.matrix(as.data.frame(samp))
colnames(samp) <- fe
rownames(samp) <- NULL
return(samp)
}
# make data with random intercept and two interacting continuous fixed effects
ngroups <- 10
nobs <- 100
beta0 <- 5
beta1 <- 1
beta2 <- 2
beta3 <- -3
dat <- data.frame(x1 = runif(nobs, -1, 1),
x2 = runif(nobs, -1, 1),
mygroup = rep(seq_len(ngroups), nobs / ngroups)) %>%
mutate(y = beta0 +
(beta1 * x1) +
(beta2 * x2) +
(beta3 * x1 * x2) +
(rnorm(ngroups)[mygroup]) +
rnorm(nobs, sd = 0.1),
x2_cat=factor(cut(x2, breaks = 5)))
summary(dat)
# plot it to see interaction in generated data
ggplot(dat, aes(x1, y, group=mygroup)) + geom_point() +
geom_smooth(method="lm", se=F) + facet_wrap(~x2_cat)
# run inla model and specify config=T for posterior sampling
form1 <- y ~ 1 + x1 * x2 + f(mygroup, model="iid")
res1 <- inla(form1, data = as.data.frame(dat),
control.compute=list(config=T))
summary(res1) # looks like mean param estimates match input values
# get samples from fixed effect posteriors
nsim <- 1000
samps <- get_fixed_samples(mod=res1, ss=nsim)
# make prediction covariate matrix that has same columns as sample matrix
newdat <- expand.grid(`(Intercept)`=1,
x1=seq(-1, 1, length.out=20),
x2=seq(-1, 1, length.out=20)) %>%
mutate(`x1:x2`=x1 * x2) %>% as.matrix()
head(samps)
head(newdat)
# make matrix for the predictions and fill it
fitmat <- matrix(nrow=nrow(newdat), ncol=nsim)
for(i in 1:nsim) fitmat[,i] <- newdat %*% samps[i,]
# summarize prediction matrix
preds <- as.data.frame(newdat) %>%
mutate(mean=apply(fitmat, 1, mean),
lcl=apply(fitmat, 1, quantile, prob=0.025),
ucl=apply(fitmat, 1, quantile, prob=0.975),
x2_cat=factor(cut(x2, breaks = 5))) %>%
arrange(x1)
# plot predictions that demonstrate interaction between fixed effects
ggplot(preds %>% group_by(x1,x2_cat) %>% sample_n(1)) +
geom_linerange(aes(x=x1, ymin=lcl, ymax=ucl, color=x2_cat), alpha=0.8) +
geom_point(aes(x=x1, y=mean, color=x2_cat), alpha=0.8) # +
# facet_wrap(~x2_cat)
ggplot(preds, aes(x=x1, y=x2, z=mean)) + geom_contour_filled()
# make an interactive version of the interaction (best viewed in RStudio)
plot_ly() %>% add_surface(x=~unique(preds$x1), y=~unique(preds$x2),
z=matrix(preds$mean, nrow=length(unique(preds$x2))))
demo(Bym)
Bym.map(result1.bym$summary.random$region$mean)
?Bym.map
load("~/flusion/runs/initial6.RData")
Bym.map(Joint.mod$summary.random$Region.1.nv$mean)
View(Bym.map)
